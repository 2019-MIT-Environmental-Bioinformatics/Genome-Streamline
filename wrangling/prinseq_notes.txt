110719 Zac

In the paper they use prinseq for quality filtering of the metagenomes before recruitment. The do entropy filtering for just the pyrosequencing data,
so I have to have a way of identifying those. I went through the supplemental table and went to all of the folders that had pyro data, copy pasted the 
SRR/ERR numbers into a text file. I am going to build this into the loop in the prinseq script to apply the entropy filtering just to these files. 

All of the runs have forward and reverse (_1 and _2), except for the fosmids and GS000 and GS001 which are just in regular fasta format from genbank. The pyro data reverse
reads are just 4 bp tags. They sanger data a long full length reads, but I think I can treat everything as its own read and not worry about mating pairs.
Some of the pyro data has _3 or _4 files too, I think these are orphaned reads or something? Not sure. Most of this extraneous stuff will get filtered 
out during prinseq because it filters out reads shorter than 100 bp.

The filtering and concatenating seems to have worked with the exception of GS000, GS001, and NESAP samples. These are all fasta instead of fastq, but so is HF10 in HOT and that seemed to work fine.
The NESAP sequences have a newline between each record, that could have something to do with it. GS000 and GS001 don't, but their headers have a bunch of slashes.

In the log file it looks like the error was that it's not in fastq format. That's why I didn't see the error for HOT, because there were all the other 454 reads that were masking the fact that
the fosmid sequences never made it in. Going to have to do another if statement in the slurm script. 

Checking to see if the concatenation worked:

The log says that from the six fastq files in GS394, there were 310911 good reads. When I zcat | grep -c, there are: 310911. Cool! So it seems to have worked. Now I just have to figure out
how to modify my SLURM script to accommodate those pesky fasta files.   
