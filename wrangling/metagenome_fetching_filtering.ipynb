{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the metagenomic data\n",
    "\n",
    "Table S5 has all of the metagenomes listed, but it does not provide SRA accession numbers. The only identifying information is a sample number. Most of the data is from the Global Ocean Sampling project from JCVI. Rather than going through one by one and looking up all associated SRAs for each GS number, I'm looking through GenBank BioProjects by searching for sample names matching those in Table S5. The code below is parsing a table of all SRA accessions associated with particular projects, and then parsing the sample numbers from table S5 to identify which ones to download. The random metagenomes not part of GOS will likely be added to the download list manually.\n",
    "\n",
    "I saved the summary from BioProject 294826 as 'GOS_294826_SraRunTable.txt' to my Desktop and transfered to the wrangling folder within our repo. This file contains all of the metadat of the SRA accessions associated with this BioProject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GS023', 'GS025', 'GS026', 'GS028', 'GS031']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOS_294826 = [] #initialize list\n",
    "with open('GOS_294826_SraRunTable.txt','r') as f: #read file\n",
    "    next(f) #skip header\n",
    "    for line in f: #loop through lines\n",
    "        GOS_294826.append(line.strip().split(',')[1].split('_')[2]) #append sample names\n",
    "GOS_294826[0:5] #print first five sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GOS_294826) #get length of sample names list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, this is the format we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 270 samples, but I'm pretty sure many of the samples have more than one accession associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(GOS_294826)) #use set to get only unique names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. There are 202 unique samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if I make a dictionary that maps the sample number to the accession....\n",
    "\n",
    "The code below should get all of the accessions for all of the samples as long as it is for the 0.1 um fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GS023': ['ERR833296'],\n",
       " 'GS026': ['ERR833300', 'ERR833299'],\n",
       " 'GS028': ['ERR833302'],\n",
       " 'GS031': ['ERR833306'],\n",
       " 'GS032': ['ERR833307', 'ERR833308'],\n",
       " 'GS033': ['ERR833310', 'ERR833309'],\n",
       " 'GS035': ['ERR833312'],\n",
       " 'GS036': ['ERR833313'],\n",
       " 'GS038': ['ERR833315'],\n",
       " 'GS040': ['ERR833317'],\n",
       " 'GS042': ['ERR833319'],\n",
       " 'GS047': ['ERR833325'],\n",
       " 'GS049': ['ERR833329'],\n",
       " 'GS051': ['ERR833331'],\n",
       " 'GS053': ['ERR833333'],\n",
       " 'GS055': ['ERR833335'],\n",
       " 'GS058': ['ERR833337', 'ERR833339', 'ERR833338', 'ERR833340'],\n",
       " 'GS061': ['ERR833343'],\n",
       " 'GS062': ['ERR833344'],\n",
       " 'GS066': ['ERR833348'],\n",
       " 'GS069': ['ERR833352', 'ERR833354', 'ERR833351', 'ERR833353'],\n",
       " 'GS071': ['ERR833356'],\n",
       " 'GS073': ['ERR833358'],\n",
       " 'GS075': ['ERR833360'],\n",
       " 'GS077': ['ERR833362'],\n",
       " 'GS079': ['ERR833364'],\n",
       " 'GS082': ['ERR833366'],\n",
       " 'GS083': ['ERR833368', 'ERR833367'],\n",
       " 'GS084': ['ERR833370',\n",
       "  'ERR833371',\n",
       "  'ERR833373',\n",
       "  'ERR833369',\n",
       "  'ERR833372',\n",
       "  'ERR833374'],\n",
       " 'GS088': ['ERR833378', 'ERR833379', 'ERR833376', 'ERR833377', 'ERR833380'],\n",
       " 'GS089': ['ERR833381', 'ERR833383', 'ERR833382'],\n",
       " 'GS091': ['ERR833385'],\n",
       " 'GS094': ['ERR833387'],\n",
       " 'GS098': ['ERR833388'],\n",
       " 'GS100': ['ERR833392', 'ERR833393'],\n",
       " 'GS102': ['ERR833394'],\n",
       " 'GS108': ['ERR833400', 'ERR833401'],\n",
       " 'GS110': ['ERR833408', 'ERR833409'],\n",
       " 'GS111': ['ERR833415'],\n",
       " 'GS112': ['ERR833418', 'ERR833419'],\n",
       " 'GS113': ['ERR833424', 'ERR833425'],\n",
       " 'GS114': ['ERR833427',\n",
       "  'ERR833429',\n",
       "  'ERR833431',\n",
       "  'ERR833426',\n",
       "  'ERR833428',\n",
       "  'ERR833430'],\n",
       " 'GS115': ['ERR833435', 'ERR833434'],\n",
       " 'GS116': ['ERR833437', 'ERR833436'],\n",
       " 'GS121': ['ERR833449'],\n",
       " 'GS122': ['ERR833450', 'ERR833451'],\n",
       " 'GS123': ['ERR833456'],\n",
       " 'GS124': ['ERR833458', 'ERR833457'],\n",
       " 'GS126': ['ERR833460', 'ERR833461'],\n",
       " 'GS130': ['ERR833465'],\n",
       " 'GS142': ['ERR833479', 'ERR833480'],\n",
       " 'GS145': ['ERR833483'],\n",
       " 'GS148': ['ERR833487', 'ERR833488'],\n",
       " 'GS203': ['ERR833496'],\n",
       " 'GS215': ['ERR833502', 'ERR833500', 'ERR833501'],\n",
       " 'GS216': ['ERR833503'],\n",
       " 'GS220': ['ERR833507'],\n",
       " 'GS221': ['ERR833509', 'ERR833508'],\n",
       " 'GS222': ['ERR833510'],\n",
       " 'GS223': ['ERR833511'],\n",
       " 'GS224': ['ERR833512'],\n",
       " 'GS238': ['ERR833515'],\n",
       " 'GS239': ['ERR833516'],\n",
       " 'GS243': ['ERR833520'],\n",
       " 'GS246': ['ERR833523'],\n",
       " 'GS249': ['ERR833525'],\n",
       " 'GS250': ['ERR833526'],\n",
       " 'GS251': ['ERR833527'],\n",
       " 'GS254': ['ERR833530'],\n",
       " 'GS262': ['ERR833539'],\n",
       " 'GS263': ['ERR833540'],\n",
       " 'GS264': ['ERR833543'],\n",
       " 'GS267': ['ERR833546'],\n",
       " 'GS270': ['ERR833549'],\n",
       " 'GS271': ['ERR833551'],\n",
       " 'GS278': ['ERR833554'],\n",
       " 'GS279': ['ERR833555'],\n",
       " 'GS281': ['ERR833557'],\n",
       " 'GS285': ['ERR833561'],\n",
       " 'GS299': ['ERR833563'],\n",
       " 'GS300': ['ERR833566'],\n",
       " 'GS302': ['ERR833571'],\n",
       " 'GS306': ['ERR833578'],\n",
       " 'GS312': ['ERR833590'],\n",
       " 'GS320': ['ERR833592'],\n",
       " 'GS323': ['ERR833601'],\n",
       " 'GS324': ['ERR833604'],\n",
       " 'GS326': ['ERR833608'],\n",
       " 'GS327': ['ERR833610'],\n",
       " 'GS002': ['ERR833272'],\n",
       " 'GS003': ['ERR833273'],\n",
       " 'GS004': ['ERR833274'],\n",
       " 'GS005': ['ERR833275'],\n",
       " 'GS006': ['ERR833276'],\n",
       " 'GS007': ['ERR833277'],\n",
       " 'GS008': ['ERR833278'],\n",
       " 'GS010': ['ERR833280'],\n",
       " 'GS011': ['ERR833282'],\n",
       " 'GS012': ['ERR833283', 'ERR833284'],\n",
       " 'GS013': ['ERR833285'],\n",
       " 'GS014': ['ERR833286'],\n",
       " 'GS015': ['ERR833287'],\n",
       " 'GS016': ['ERR833288'],\n",
       " 'GS017': ['ERR833289'],\n",
       " 'GS018': ['ERR833290'],\n",
       " 'GS019': ['ERR833292'],\n",
       " 'GS020': ['ERR833293'],\n",
       " 'GS021': ['ERR833294'],\n",
       " 'GS022': ['ERR833295'],\n",
       " 'GS027': ['ERR833301'],\n",
       " 'GS029': ['ERR833303'],\n",
       " 'GS030': ['ERR833304', 'ERR833305'],\n",
       " 'GS034': ['ERR833311'],\n",
       " 'GS037': ['ERR833314'],\n",
       " 'GS039': ['ERR833316'],\n",
       " 'GS041': ['ERR833318'],\n",
       " 'GS043': ['ERR833320'],\n",
       " 'GS044': ['ERR833321'],\n",
       " 'GS045': ['ERR833323'],\n",
       " 'GS046': ['ERR833324'],\n",
       " 'GS048': ['ERR833326'],\n",
       " 'GS050': ['ERR833330'],\n",
       " 'GS052': ['ERR833332'],\n",
       " 'GS054': ['ERR833334'],\n",
       " 'GS057': ['ERR833336'],\n",
       " 'GS060': ['ERR833342'],\n",
       " 'GS063': ['ERR833345'],\n",
       " 'GS067': ['ERR833349'],\n",
       " 'GS070': ['ERR833355'],\n",
       " 'GS072': ['ERR833357'],\n",
       " 'GS074': ['ERR833359'],\n",
       " 'GS076': ['ERR833361'],\n",
       " 'GS078': ['ERR833363'],\n",
       " 'GS080': ['ERR833365'],\n",
       " 'GS086': ['ERR833375'],\n",
       " 'GS093': ['ERR833386'],\n",
       " 'GS099': ['ERR833389', 'ERR833391', 'ERR833390'],\n",
       " 'GS103': ['ERR833395'],\n",
       " 'GS109': ['ERR833406'],\n",
       " 'GS117': ['ERR833439', 'ERR833440'],\n",
       " 'GS119': ['ERR833447'],\n",
       " 'GS120': ['ERR833448'],\n",
       " 'GS125': ['ERR833459'],\n",
       " 'GS128': ['ERR833464'],\n",
       " 'GS132': ['ERR833467'],\n",
       " 'GS134': ['ERR833470'],\n",
       " 'GS136': ['ERR833473'],\n",
       " 'GS137': ['ERR833474'],\n",
       " 'GS138': ['ERR833475'],\n",
       " 'GS139': ['ERR833476'],\n",
       " 'GS140': ['ERR833477'],\n",
       " 'GS144': ['ERR833482'],\n",
       " 'GS146': ['ERR833484'],\n",
       " 'GS147': ['ERR833485'],\n",
       " 'GS149': ['ERR833490', 'ERR833491'],\n",
       " 'GS201': ['ERR833493'],\n",
       " 'GS202': ['ERR833495'],\n",
       " 'GS204': ['ERR833497'],\n",
       " 'GS205': ['ERR833498'],\n",
       " 'GS217': ['ERR833504'],\n",
       " 'GS218': ['ERR833505'],\n",
       " 'GS219': ['ERR833506'],\n",
       " 'GS226': ['ERR833513'],\n",
       " 'GS240': ['ERR833517'],\n",
       " 'GS241': ['ERR833518'],\n",
       " 'GS242': ['ERR833519'],\n",
       " 'GS244': ['ERR833521'],\n",
       " 'GS247': ['ERR833524'],\n",
       " 'GS253': ['ERR833529'],\n",
       " 'GS257': ['ERR833531'],\n",
       " 'GS258': ['ERR833532'],\n",
       " 'GS259': ['ERR833534'],\n",
       " 'GS260': ['ERR833535'],\n",
       " 'GS265': ['ERR833544'],\n",
       " 'GS266': ['ERR833545'],\n",
       " 'GS268': ['ERR833547'],\n",
       " 'GS269': ['ERR833548'],\n",
       " 'GS272': ['ERR833552'],\n",
       " 'GS277': ['ERR833553'],\n",
       " 'GS280': ['ERR833556'],\n",
       " 'GS282': ['ERR833558'],\n",
       " 'GS283': ['ERR833559'],\n",
       " 'GS284': ['ERR833560'],\n",
       " 'GS286': ['ERR833562'],\n",
       " 'GS301': ['ERR833569'],\n",
       " 'GS305': ['ERR833576'],\n",
       " 'GS308': ['ERR833584'],\n",
       " 'GS309': ['ERR833586'],\n",
       " 'GS310': ['ERR833588'],\n",
       " 'GS311': ['ERR833589'],\n",
       " 'GS313': ['ERR833591'],\n",
       " 'GS321': ['ERR833595'],\n",
       " 'GS325': ['ERR833607'],\n",
       " 'GS090': ['ERR833384'],\n",
       " 'GS237': ['ERR833514'],\n",
       " 'GS009': ['ERR833279'],\n",
       " 'GS252': ['ERR833528'],\n",
       " 'GS307': ['ERR833581'],\n",
       " 'GS322': ['ERR833598']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOS_294826_dict = {} #initialize empty dictionary\n",
    "with open('GOS_294826_SraRunTable.txt','r') as f: #open file\n",
    "    next(f) #skip header\n",
    "    for line in f: #loop through lines\n",
    "        if line.strip().split(',')[1].split('_')[3] == '0.1': #if the size fraction in 0.1, proceed (we want to keep these). inferred naming convention indicates size fraction\n",
    "            samp = line.strip().split(',')[1].split('_')[2] #get SRA accession number\n",
    "            if samp in GOS_294826_dict.keys(): #if sample already has at least one SRA\n",
    "                GOS_294826_dict[samp].append(line.strip().split(',')[0]) #append to dictionary value\n",
    "            else: #if this is the first time sample is seend\n",
    "                GOS_294826_dict[samp] = [line.strip().split(',')[0]] #create new dictionary value\n",
    "        else: #if size fraction is not equal to 0.1\n",
    "            pass  #don't do anything\n",
    "GOS_294826_dict #print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GOS_294826_dict.keys()) #print length of dictionary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we've got what we want. A dictionary with 199 keys, one for each sample, pointing to values that are the accession numbers. Now I want to read in Table S5, parse the sample names, and find which ones are not in the GOS_29586_dict.\n",
    "\n",
    "I used a free online tool to extract the information from the supplental table into a csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GS123', 'GS122', 'GS121', 'GS120', 'GS119']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Swan_samps = [] #create list to hold all the sample names we want\n",
    "with open('Table_S5.csv','r') as f: #open the csv\n",
    "    next(f) #skip the header\n",
    "    for line in f: #loop over the lines \n",
    "        Swan_samps.append(line.strip().split(',')[0].strip('*')) #append sample name to list\n",
    "Swan_samps[0:5] #print first five"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so we've got our list of samples from the paper. Now I want to find which ones are found in the GOS_294826_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GS002',\n",
       " 'GS003',\n",
       " 'GS004',\n",
       " 'GS005',\n",
       " 'GS006',\n",
       " 'GS007',\n",
       " 'GS008',\n",
       " 'GS009',\n",
       " 'GS010',\n",
       " 'GS013',\n",
       " 'GS014',\n",
       " 'GS015',\n",
       " 'GS016',\n",
       " 'GS017',\n",
       " 'GS018',\n",
       " 'GS019',\n",
       " 'GS021',\n",
       " 'GS022',\n",
       " 'GS023',\n",
       " 'GS026',\n",
       " 'GS027',\n",
       " 'GS028',\n",
       " 'GS029',\n",
       " 'GS030',\n",
       " 'GS031',\n",
       " 'GS034',\n",
       " 'GS035',\n",
       " 'GS036',\n",
       " 'GS037',\n",
       " 'GS048',\n",
       " 'GS049',\n",
       " 'GS051',\n",
       " 'GS108',\n",
       " 'GS109',\n",
       " 'GS110',\n",
       " 'GS111',\n",
       " 'GS112',\n",
       " 'GS113',\n",
       " 'GS114',\n",
       " 'GS115',\n",
       " 'GS116',\n",
       " 'GS117',\n",
       " 'GS119',\n",
       " 'GS120',\n",
       " 'GS121',\n",
       " 'GS122',\n",
       " 'GS123',\n",
       " 'GS148',\n",
       " 'GS149'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Swan_samps).intersection(set(GOS_294826_dict.keys())) #find out which of the samples you have already retrieved by looking for intersection between the two sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so here are all of the samples in common. But how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(Swan_samps).intersection(set(GOS_294826_dict.keys()))) #find the length of the intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49 out of the total 115. Not bad. Now I am going to find which don't intersect. Pick some GS numbers and search SRA for the project number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ECH1_4444_77',\n",
       " 'ECH2_4444_83',\n",
       " 'ECH3_4445_65',\n",
       " 'ECH4_4445_66',\n",
       " 'ECH5_4445_67',\n",
       " 'ECH6_4445_68',\n",
       " 'ECH7_4445_69',\n",
       " 'ECH8_4445_70',\n",
       " 'ERS095011',\n",
       " 'ERS095012',\n",
       " 'ERS095013',\n",
       " 'ERS095014',\n",
       " 'ERS095015',\n",
       " 'ERS095018',\n",
       " 'ERS095019',\n",
       " 'GS000b',\n",
       " 'GS000c',\n",
       " 'GS000d',\n",
       " 'GS001a',\n",
       " 'GS001b',\n",
       " 'GS001c',\n",
       " 'GS025',\n",
       " 'GS108_454',\n",
       " 'GS112_454',\n",
       " 'GS235',\n",
       " 'GS236',\n",
       " 'GS346',\n",
       " 'GS347',\n",
       " 'GS348',\n",
       " 'GS349',\n",
       " 'GS351',\n",
       " 'GS352',\n",
       " 'GS353',\n",
       " 'GS355',\n",
       " 'GS357',\n",
       " 'GS358',\n",
       " 'GS359',\n",
       " 'GS360',\n",
       " 'GS362',\n",
       " 'GS363',\n",
       " 'GS364',\n",
       " 'GS366',\n",
       " 'GS367',\n",
       " 'GS368',\n",
       " 'GS369',\n",
       " 'GS386',\n",
       " 'GS387',\n",
       " 'GS388',\n",
       " 'GS389',\n",
       " 'GS390',\n",
       " 'GS391',\n",
       " 'GS392',\n",
       " 'GS393',\n",
       " 'GS394',\n",
       " 'HF10',\n",
       " 'HOT215',\n",
       " 'MED',\n",
       " 'P12_a',\n",
       " 'P12_f',\n",
       " 'P12_j',\n",
       " 'P26_a',\n",
       " 'P26_j',\n",
       " 'P4_a',\n",
       " 'P4_f',\n",
       " 'P4_j'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Swan_samps) - set(GOS_294826_dict.keys()) #find which from the paper are missing from the intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there is a duplicate in the Swan_samps. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(Swan_samps)) #print length unique sample names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Swan_samps) #print length of sample names list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the list is one longer than the length of the set (which has no dups). How to find which is the duplicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GS346 is the duplicate!\n",
      "GS346 is the duplicate!\n"
     ]
    }
   ],
   "source": [
    "for i in Swan_samps: #loop over elements in sample name list\n",
    "    if Swan_samps.count(i)>1: #if seen more than once\n",
    "        print(i+\" is the duplicate!\") #print which one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so we've found the duplicate. Consulting the table, it looks like this was accidentally added to the table and doesn't actually represent an additional sample. This is substantiated by its absence on Fig. S10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're off to look for the remaining 64 samples. Starting with other GOS samples. It looks like many of them are in BioProject 50699. The SRA table is in a different format, so will have to modify code from above. There is no information in the metadata for this BioProject to indicate what size fraction these data are from, so I'm just going to have to assume it is 0.1 um.\n",
    "\n",
    "I downloaded the BioProject summary as I did before and uploaded to poseidon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GS377': ['SRR063386', 'SRR062185', 'SRR062246'],\n",
       " 'GS379': ['SRR063387', 'SRR062187', 'SRR062249'],\n",
       " 'GS392': ['SRR063388', 'SRR063770', 'SRR062260', 'SRR062261'],\n",
       " 'GS375': ['SRR063389', 'SRR062243', 'SRR062183'],\n",
       " 'GS235': ['SRR063767', 'SRR062131', 'SRR062132'],\n",
       " 'GS391': ['SRR063769', 'SRR062258', 'SRR062259', 'SRR063390'],\n",
       " 'GS233': ['SRR061742', 'SRR062130', 'SRR062199'],\n",
       " 'GS394': ['SRR062129', 'SRR062198', 'SRR062264'],\n",
       " 'GS236': ['SRR062134', 'SRR062201', 'SRR063385'],\n",
       " 'GS346': ['SRR062135', 'SRR062202', 'SRR062203'],\n",
       " 'GS347': ['SRR062139', 'SRR062204', 'SRR062205', 'SRR062206'],\n",
       " 'GS348': ['SRR062140', 'SRR062141'],\n",
       " 'GS349': ['SRR062142', 'SRR062207', 'SRR062208'],\n",
       " 'GS350': ['SRR062143', 'SRR062144', 'SRR062209'],\n",
       " 'GS351': ['SRR062145', 'SRR062146', 'SRR062210', 'SRR062211'],\n",
       " 'GS352': ['SRR062147', 'SRR062212', 'SRR062213'],\n",
       " 'GS353': ['SRR062148', 'SRR062214', 'SRR062625'],\n",
       " 'GS354': ['SRR062149', 'SRR062216', 'SRR062215'],\n",
       " 'GS355': ['SRR062150', 'SRR062151', 'SRR062217'],\n",
       " 'GS356': ['SRR062152', 'SRR062218', 'SRR062219'],\n",
       " 'GS357': ['SRR062153', 'SRR062154', 'SRR062220', 'SRR062221'],\n",
       " 'GS358': ['SRR062155', 'SRR062222', 'SRR062223'],\n",
       " 'GS359': ['SRR062156', 'SRR062224', 'SRR062225', 'SRR062226', 'SRR062227'],\n",
       " 'GS361': ['SRR062158', 'SRR062159', 'SRR062230'],\n",
       " 'GS362': ['SRR062160', 'SRR062231', 'SRR062232'],\n",
       " 'GS363': ['SRR062161', 'SRR062162', 'SRR062233'],\n",
       " 'GS364': ['SRR062163', 'SRR062234', 'SRR062235'],\n",
       " 'GS365': ['SRR062164', 'SRR062165', 'SRR062236'],\n",
       " 'GS366': ['SRR062166', 'SRR062237', 'SRR062275'],\n",
       " 'GS367': ['SRR062167', 'SRR062168', 'SRR062238'],\n",
       " 'GS368': ['SRR062169', 'SRR062170', 'SRR062239'],\n",
       " 'GS369': ['SRR062171', 'SRR062172', 'SRR062240'],\n",
       " 'GS370': ['SRR062173', 'SRR062174', 'SRR062241'],\n",
       " 'GS371': ['SRR062175', 'SRR062176', 'SRR062177'],\n",
       " 'GS372': ['SRR062178', 'SRR062179', 'SRR062180'],\n",
       " 'GS374': ['SRR062182', 'SRR062181', 'SRR063766'],\n",
       " 'GS376': ['SRR062184', 'SRR062244', 'SRR063771'],\n",
       " 'GS386': ['SRR062188', 'SRR062250', 'SRR062251'],\n",
       " 'GS387': ['SRR062189', 'SRR062252', 'SRR062268'],\n",
       " 'GS388': ['SRR062190', 'SRR062253', 'SRR062254'],\n",
       " 'GS389': ['SRR062191', 'SRR062192', 'SRR062255'],\n",
       " 'GS393': ['SRR062196', 'SRR062197', 'SRR062262', 'SRR062263'],\n",
       " 'GS360': ['SRR062228', 'SRR062229', 'SRR062157'],\n",
       " 'GS378': ['SRR062247', 'SRR062248', 'SRR063768'],\n",
       " 'GS390': ['SRR062256', 'SRR062257', 'SRR062193']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOS_50699_dict = {} #initialize dictionary\n",
    "with open('GOS_50699_SraRunTable.txt','r') as f: #open file\n",
    "    next(f) #skip header\n",
    "    for line in f: #loop over lines\n",
    "        samp = \"GS\"+line.strip().split(',')[3][-3:] #retrieve sample name\n",
    "        if samp in GOS_50699_dict.keys(): #if already seen\n",
    "            GOS_50699_dict[samp].append(line.strip().split(',')[0]) #append SRA to dictionary value\n",
    "        else: #otherwise\n",
    "            GOS_50699_dict[samp] = [line.strip().split(',')[0]] #assign new value\n",
    "GOS_50699_dict #print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay cool so we've got this second dictionary. These were all done with 454 sequencing, so keep that in mind later on when using PRINSEQ (entropy filtering stuff only applies to pyrosequencing...)\n",
    "\n",
    "Anyway, now I want to merge the two GOS dictionaries and then see which Swan samples are still missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict = GOS_294826_dict #copy GOS_294826_dict to new object to map all samples to their SRA accesion number(s)\n",
    "samp2acc_dict.update(GOS_50699_dict) #update the dictionary with the other GOS dict\n",
    "samp2acc_dict #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(Swan_samps) - set(samp2acc_dict.keys()) #print those that are still missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great we now most only have the non-GOS! The ERS* samples are all from one location, listed in Fig. S10 as HI. I think these are from Heligoland? These are represented as one sample, so will be pooled. I am going to manually place these into the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict['HI'] = ['ERS095011',\n",
    " 'ERS095012',\n",
    " 'ERS095013',\n",
    " 'ERS095014',\n",
    " 'ERS095015',\n",
    " 'ERS095018',\n",
    " 'ERS095019'] #update the samp2acc_dict manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all these randos I have to track down. And then there are the lingering GOS...\n",
    "\n",
    "I was able to track down the ECH samples as coming from:\n",
    "\n",
    "Gilbert, J. A., Meyer, F., Schriml, L., Joint, I. R., Mühling, M., & Field, D. (2010). Metagenomes and metatranscriptomes from the L4 long-term coastal monitoring station in the Western English Channel. Standards in genomic sciences, 3(2), 183.\n",
    "\n",
    "This was based just on a Google Scholar search for 'english channel metagenomic 454'. It works...\n",
    "\n",
    "As a note, I only selected those that were labeled as WGS, not those that were EST. This reduced it to ten samples (greater than 8 because I think for some samples they sequenced the prefilters??) I was able to identify the two odd samples by looking at BioSample metadata. \n",
    "\n",
    "I exported from GenBank a list of all associated SRAs into a file called \"ECH_acc.txt\" and uploaded to Poseidon.\n",
    "\n",
    "There should be a total of 89 keys in dictionary when you are done (number of columns in Fig. S10)!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ECH_acc.txt','r') as f: #open file\n",
    "    samp2acc_dict['ECH'] = [] #initialize new key value in sample/accession map\n",
    "    for line in f: #loop through lines in file\n",
    "        samp2acc_dict['ECH'].append(line.strip()) #append to value in dictionary\n",
    "samp2acc_dict['ECH'] #print value of ECH key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Who's left? The GS000 and GS001 samples aren't in the SRA, apparently. I just found them on iMicrobe and am downloading the sequence files manually. So those don't have to be included in the dictionary. These files do not actually contain the reads but rather have IDs that can be used to capture the desired reads from a huge file with all the reads from this iMicrobe project.\n",
    "\n",
    "Now I want to add the 454 sequencing for GS108 and GS112. Swan also included the 0.8 and 3um fractions for these samples, as well as for GS048. Adding all now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict['GS108'] #print SRA number already retrieved for this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOS108_add = ['ERR833396','ERR833397','ERR833398','ERR833399','SRR066138','ERR833403','ERR833402'] #list of SRA numbers to add (found manually)\n",
    "for i in GOS108_add: #for each element in the list of SRAs to add\n",
    "    samp2acc_dict['GS108'].append(i) #append them to the dictionary key value\n",
    "samp2acc_dict['GS108'] #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOS112_add = ['SRR066139','ERR833417','ERR833416','ERR833420','ERR833421'] #same as above\n",
    "for i in GOS112_add:\n",
    "    samp2acc_dict['GS112'].append(i)\n",
    "samp2acc_dict['GS112']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOS048_add = ['ERR833327','ERR833328'] #same as above\n",
    "for i in GOS048_add:\n",
    "    samp2acc_dict['GS048'].append(i)\n",
    "samp2acc_dict['GS048']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict['GS025'] = ['ERR833298'] #same as above except just one SRA accession to add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now I just need:\n",
    "\n",
    " 'HF10',\n",
    " 'HOT215',\n",
    " 'MED',\n",
    " 'P12_a',\n",
    " 'P12_f',\n",
    " 'P12_j',\n",
    " 'P26_a',\n",
    " 'P26_j',\n",
    " 'P4_a',\n",
    " 'P4_f',\n",
    " 'P4_j'\n",
    " \n",
    "Time to sleuth so more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HF10 and HOT215 are the two Hawaii samples. HOT215 has four matches to BioSamples. Getting 454 data from all four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict['HOT'] = [\"SRR1303811\"] #add manually\n",
    "samp2acc_dict['HOT'] #print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HF10 data is not in the SRA, but just in GenBank in nucleotide format. Acccessions are DU731018-DU796676 and DU800850-DU800864. This info is at the very end of:\n",
    "\n",
    "DeLong, E. F., Preston, C. M., Mincer, T., Rich, V., Hallam, S. J., Frigaard, N. U., ... & Chisholm, S. W. (2006). Community genomics among stratified microbial assemblages in the ocean's interior. Science, 311(5760), 496-503.\n",
    "\n",
    "Will retrieve later. Need to figure out which ones are from surface sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found 'MED' by searching 'mediterranean metagenomics 454'. Published in:\n",
    "\n",
    "Ghai, R., Martin-Cuadrado, A. B., Molto, A. G., Heredia, I. G., Cabrera, R., Martin, J., ... & Mira, A. (2010). Metagenome of the Mediterranean deep chlorophyll maximum studied by direct and fosmid library 454 pyrosequencing. The ISME journal, 4(9), 1154.\n",
    "\n",
    "Accession is SRR037008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict['MED'] = ['SRR037008'] #add manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to get the one that start with P. These are all from around the same area and in Fig. S10 are preceded by NESAP. These also are fosmids. The sequences are on GenBank. Accession numbers KG088956–KG619837. This is from:\n",
    "\n",
    "Wright, J. J., Mewis, K., Hanson, N. W., Konwar, K. M., Maas, K. R., & Hallam, S. J. (2014). Genomic properties of Marine Group A bacteria indicate a role in the marine sulfur cycle. The ISME journal, 8(2), 455.\n",
    "\n",
    "Also in some sort of other format (Genome Survey Sequences?) Accession LIBGSS_039072–LIBGSS_039117. \n",
    "\n",
    "Will do this later.\n",
    "\n",
    "Okay! So that should do it. Now I just have to extract the keys that I want from the samp2acc_dict, because a lot of the keys are for samples that aren't included in Swan. \n",
    "\n",
    "I just had to type out all of the samples from Fig. S10 so I had them in the right order for making the figure later. I can now use that to pull the accession numbers I need from the samp2acc_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swan_stations = [] #initialize \n",
    "with open('sample_stations.txt','r') as f: #open list of metagenome stations in order\n",
    "    for line in f: #loop through\n",
    "        swan_stations.append(line.strip()) #append to list\n",
    "swan_stations #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict.keys() #print names of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp2acc_dict_final = {} #initialize final sample to accession map\n",
    "for i in swan_stations: #loop through\n",
    "    if i in list(samp2acc_dict.keys()): #if it appears in the original sample to accesion map\n",
    "        samp2acc_dict_final[i] = samp2acc_dict[i] #update\n",
    "    else: #otherwise\n",
    "        pass #do nothing\n",
    "samp2acc_dict_final #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(samp2acc_dict_final.keys())) #get number of samples in final sample accession map. just double checking everything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(swan_stations) #get number of stations from figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samp2acc_dict) #get number of samples from original sample accession map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samp2acc_dict_final) #get number of samples in final sample accession map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay this is what we expect. There are five stations missing. These are GS000 and GS001, because these I downloaded through iMicrobe. The other three are the NESAP P4, P12 and P26. These are fosmids that I need to get from GenBank. I also need to add the fosmid sequences to HOT. \n",
    "\n",
    "Okay now I can actually use this dictionary somehow to download all of these files from SRA. I am just going to write this dictionary to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('acc_dict.txt','w') #write a file that we will pass into a function retrieving the accession we want\n",
    "for key in samp2acc_dict_final: #for each key in the final sample accesion map \n",
    "    output.write(key+'\\t'+','.join(map(str,samp2acc_dict_final[key]))+'\\n') #write the sample name and then a comma-separated list of all of the associated accessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am going to work on getting the fosmid sequences. First I'll start with the HF10 from HOT. Acccessions are DU731018-DU796676 and DU800850-DU800864. I want to put these all into a comma separated list for downloading using entrez-direct. \n",
    "\n",
    "However, it appears that these accessions refer a lot more sequences than were used in the paper (7,829). There is an identifier in the GenBank record HF10_10-07-02. It is only these samples that we want (I think). Going with just HF10. Saving GenBank record in accession list format. Renamed HF10_acc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can actually use this file to download the sequences. I wrote a SLURM script that loops over this file and uses efetch to get the sequences in fasta format. Saved into a file called HF10.fa in /vortexfs1/omics/env-bio/collaboration/genome-streamlining/data/metagenomes/HOT/fosmid\n",
    "\n",
    "Next order of business is to get the NESAP data. Then I need to figure out how to extract the reads I want from the GOS_reads that correspond to GS000 and GS001. More on that later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay it is really hard to figure out which of the fosmid libraries from Wright et al they actually used. I'm going to sit on this for a while and pick up later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the supplementary table from Wright et al, I looked up the GSS accessions, which are actually BioSamples on NCBI. For each station and month, I pasted the accession number for the 10m depth into NCBI and retrieved an accession list of all associated nucleotides. Each of these were transferred to the cluster, and then they were concatenated together by station. This is almost certainly not the same data that was used in Swan, but it's all we got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the NESAP data, I am having a hard time downloading through the command line (really slow), so I am just downloading manually through NCBI and uploading to Poseidon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right so for the GS000 and GS001, I have a master set of reads and then each sample has three .fa files that just have the headers that point to the master reads. I have to figure out some way to grab them. The .fa files are single lines. The way to split them is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sed -i \"s/>/\\n>/g\" file.fa #I was not able to get bash to run in the notebook, so this was done in the command line, as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i flag edits the file instead of to STDOUT. So this is going to substitute (the s) every > with a newline then a >. Now it's over multiple lines. The next issue is that the headers don't actually match the master file. They are truncated. So I can probably use awk to grab just the fields I want. Maybe just the first few, including the mate? Or all the way up to where it was truncated? Whatever, I'll do this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay continuing on. I am going to use the command above to reformat the GS000 and GS001 files. I want to retain just the first 8 fields of the GS001 and GS000 files as well as just the first 8 fields of the GOS reads database. Going to do that with cut. Did it manually for each file and saved to new file with _trunc added to filename. Then deleted the original. COuldn't find an easy way to edit in place with cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cut -d ' ' -f1-9 JCVI_SMPL_1103283000002.fa > JCVI_SMPL_1103283000002_trunc.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did this for each of the 6 files. Now I just have the first 8 fields. Doing the same for GOS_reads. First unzipped with gunzip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cut -d ' ' -f1-9 GOS_reads.fa > GOS_reads_trunc.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going to use some sort of grepping to match the input files to the database and pull out the match (header) and the next line (sequence). The -A flag of grep allows you to specify number of lines trailing the match to output. Should be 1 in our case. I think there is also a way of reading in the pattern from a file. Yes that's with the -f flag. This looks through the file, one pattern per line. Going to write a slurm script because this takes a while, but the basic command will look like this (when run from the GS000/1 directories):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!grep -f file_trunc.fa -A1 ../GOS_reads/GOS_reads_trunc.fa > outfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change of plans. For some reason it isn't working the way I outlined it above. But the GOS_reads headers all have sample names in them (i.e. GS000b, GS001c, etc.) So I am just going to grep for those, printing the matching line and the following and saving output to file. Trial with this on GS001a actually yielded same number of sequences as presented in table s5. So that's good news. \n",
    "\n",
    "Example for how I did this below. Did individually for each of the six. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!grep -A1 \"GS001b\" ../GOS_reads/GOS_reads_trunc.fa > GS001b.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the numbers of sequence match up perfectly to the table. Now I am going to delete the GOS_reads folder becuase we don't need it anymore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
